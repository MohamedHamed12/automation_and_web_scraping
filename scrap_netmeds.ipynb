{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedHamed12/web_scraping/blob/main/scrap_netmeds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7d_vPk4kT9f"
      },
      "outputs": [],
      "source": [
        "!pip install google_colab_selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEuATjJ0kgud"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCSn03wbYKH9"
      },
      "outputs": [],
      "source": [
        "import google_colab_selenium as gs\n",
        "\n",
        "driver = gs.Chrome()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfM4JOsIk35h"
      },
      "outputs": [],
      "source": [
        "url='https://www.netmeds.com/'\n",
        "response = requests.get(url)\n",
        "if response.status_code != 200:\n",
        "    print(\"Failed to fetch data from the URL.\")\n",
        "    exit()\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "nav_sections = soup.find('div', class_='nav-sections')\n",
        "categories_urls = [category.find('a')['href'] for category in nav_sections.find_all('li') if category.find('a')['href']]\n",
        "\n",
        "# class=\"nav-sections\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt_5KqJS5EOj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wXSJRq2xCOM"
      },
      "outputs": [],
      "source": [
        "def save_files(image_url, folder_name, category_name):\n",
        "        # if not os.path.exists(folder_name\n",
        "        print(image_url,category_name)\n",
        "        if not os.path.exists(folder_name):\n",
        "          os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "\n",
        "        filename = f\"{category_name}.jpg\"\n",
        "        image_path = os.path.join(folder_name, filename)\n",
        "        urlretrieve(image_url, image_path)\n",
        "\n",
        "\n",
        "        data = {\n",
        "            'Category_name': category_name,\n",
        "            'Image': filename\n",
        "\n",
        "        }\n",
        "        # df = pd.DataFrame([data], index=[0])  # Add an index of 0\n",
        "\n",
        "        df = pd.DataFrame([data])\n",
        "        csv_file = 'catagory_data.csv'\n",
        "\n",
        "        if os.path.exists(csv_file):\n",
        "            df.to_csv(csv_file, mode='a', index=False, header=False)\n",
        "        else:\n",
        "            df.to_csv(csv_file, index=False, header=True)\n",
        "\n",
        "        print(\"Data extraction completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2laPFk8ivR98",
        "outputId": "cebe781f-27d7-42e4-cc90-71bf3cae78fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.netmeds.com/images/cms/aw_rbslider/slides/1715853034_Neurobion_1000x200.png covid-essentials\n",
            "Data extraction completed.\n",
            "https://www.netmeds.com/images/cms/aw_rbslider/slides/1707916702_Category_Banner_Web_Portfolio.png diabetes-support\n",
            "Data extraction completed.\n",
            "https://www.netmeds.com/images/category/v1/916/cardiac_care_5.jpg cardiac-care\n",
            "Data extraction completed.\n",
            "https://www.netmeds.com/images/cms/aw_rbslider/slides/1717675731_Sexual_Health.jpg ayurvedic\n",
            "Data extraction completed.\n",
            "https://www.netmeds.com/images/cms/aw_rbslider/slides/1716968586_1000x200_(1).jpg homeopathy\n",
            "Data extraction completed.\n",
            "https://www.netmeds.com/images/cms/aw_rbslider/slides/1695200613_Surgical.jpg surgical\n",
            "Data extraction completed.\n",
            "https://www.netmeds.com/images/cms/aw_rbslider/slides/1717676089_Skin_Care.jpg skin-care\n",
            "Data extraction completed.\n"
          ]
        }
      ],
      "source": [
        "def get_category_data(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch data from {url}\")\n",
        "        return None\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    category_name = url.split('/')[-1]\n",
        "    # Find the image element\n",
        "    image_body=soup.find('div',class_='category-main-banner')\n",
        "    if not image_body:\n",
        "        return category_name\n",
        "    image_url = [image['src'] for image in image_body.find_all('img')][0]\n",
        "    # if not image_url:\n",
        "    #     return category_name\n",
        "    save_files(image_url,'catagory',category_name)\n",
        "    return category_name\n",
        "for url in categories_urls:\n",
        "    get_category_data('https://www.netmeds.com'+url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lFtWR2LjzWms"
      },
      "outputs": [],
      "source": [
        "# !rm -rf catagory\n",
        "# !rm -rf catagory_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB5yZCT6XODw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_product_urls(category_url):\n",
        "    driver = gs.Chrome()\n",
        "    url = category_url\n",
        "\n",
        "    print(url)\n",
        "    # Open the webpage\n",
        "    driver.get(url)\n",
        "\n",
        "    # Simulate scrolling to load more content\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    while True:\n",
        "        # Scroll down to bottom\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        # Wait to load page\n",
        "        time.sleep(2)\n",
        "        # Calculate new scroll height and compare with last scroll height\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # Find all product containers\n",
        "    scope = soup.find(\"div\", class_=\"all-product\")\n",
        "    if not scope:\n",
        "        print(\"No product containers found.\")\n",
        "        return []\n",
        "    product_containers = scope.find_all(\"div\", class_=\"cat-item\")\n",
        "\n",
        "    # Iterate over each product container\n",
        "    urls= []\n",
        "    for product in product_containers:\n",
        "        url = product.find(\"a\", class_=\"category_name\")[\"href\"]\n",
        "        urls.append(url)\n",
        "    driver.quit()\n",
        "    return urls\n",
        "\n",
        "catagory_products={}\n",
        "for category_url in categories_urls:\n",
        "  urls=get_product_urls('https://www.netmeds.com'+category_url)\n",
        "  catagory_products[category_url.split('/')[-1]]=urls\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaFGDZiPZuSU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def extract_data(url,catagory_name):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extracting product name\n",
        "        product_name = soup.find('h1', class_='black-txt').text.strip()\n",
        "        # Extracting price\n",
        "        price = soup.find('span', class_='final-price').text.strip()\n",
        "\n",
        "\n",
        "        # Extracting image URLs\n",
        "        images_body=soup.find('div',class_='product-media')\n",
        "\n",
        "        image_urls = [image['src'] for image in images_body.find_all('img')]\n",
        "\n",
        "\n",
        "        # Create a folder to store images\n",
        "        folder_name = 'product_images'\n",
        "        os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "        # Download images\n",
        "        files_names = []\n",
        "\n",
        "        for i, image_url in enumerate(image_urls[:2]):\n",
        "            filename = f\"{product_name.replace(' ', '_')}_{i}.jpg\"\n",
        "            image_path = os.path.join(folder_name, filename)\n",
        "            try:\n",
        "              urlretrieve(image_url, image_path)\n",
        "            except Exception as e:\n",
        "              continue\n",
        "            files_names.append(filename)\n",
        "            # print(f\"Image {i+1} downloaded: {image_path}\")\n",
        "\n",
        "        # Save data to CSV with header\n",
        "        data = {\n",
        "            'Category': catagory_name,\n",
        "            'Product Name': product_name,\n",
        "                'Price': price,\n",
        "\n",
        "                'Images': [', '.join(files_names)]}\n",
        "\n",
        "        # df = pd.DataFrame(data)\n",
        "        # df.to_csv('product_data.csv', index=False, header=True)\n",
        "        # Append data to the CSV file\n",
        "        # new_data = {'Product Name': product_name, 'Price': price}  # Add more data here\n",
        "        df = pd.DataFrame([data])\n",
        "        if os.path.exists('product_data.csv'):\n",
        "            df.to_csv('product_data.csv', mode='a', index=False, header=False)\n",
        "        else:\n",
        "            df.to_csv('product_data.csv', index=False, header=True)\n",
        "\n",
        "        print(\"Data extraction completed.\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch data from {url}\")\n",
        "\n",
        "for catagory_name,products in catagory_products.items():\n",
        "  for url in products[0:30]:\n",
        "      extract_data('https://www.netmeds.com/'+url,catagory_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXllYvUUrSSU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def fetch_html(url):\n",
        "    \"\"\"Fetch HTML content from the given URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to fetch HTML content from {url}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxhmGxvirgCL"
      },
      "outputs": [],
      "source": [
        "def extract_data(url, category_name):\n",
        "    \"\"\"Extract product data from the provided URL.\"\"\"\n",
        "    html_content = fetch_html(url)\n",
        "    if html_content:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        product_name = soup.find('h1', class_='black-txt').text.strip()\n",
        "        price = soup.find('span', class_='final-price').text.strip()\n",
        "        images_body = soup.find('div', class_='product-media')\n",
        "        image_urls = [image['src'] for image in images_body.find_all('img')]\n",
        "        folder_name = 'product_images'\n",
        "        os.makedirs(folder_name, exist_ok=True)\n",
        "        files_names = []\n",
        "\n",
        "        for i, image_url in enumerate(image_urls[:2]):\n",
        "            filename = f\"{product_name.replace(' ', '_')}_{i}.jpg\"\n",
        "            image_path = os.path.join(folder_name, filename)\n",
        "            try:\n",
        "              urlretrieve(image_url, image_path)\n",
        "            except Exception as e:\n",
        "              continue\n",
        "            files_names.append(filename)\n",
        "\n",
        "        data = {\n",
        "            'Category': [category_name],\n",
        "            'Product Name': [product_name],\n",
        "            'Price': [price],\n",
        "            'Images': [', '.join(files_names)]\n",
        "        }\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        csv_file = 'product_data.csv'\n",
        "\n",
        "        if os.path.exists(csv_file):\n",
        "            df.to_csv(csv_file, mode='a', index=False, header=False)\n",
        "        else:\n",
        "            df.to_csv(csv_file, index=False, header=True)\n",
        "\n",
        "        # print(\"Data extraction completed.\")\n",
        "    else:\n",
        "        print(f\"Data extraction failed for URL: {url}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kJQK9aurh0T"
      },
      "outputs": [],
      "source": [
        "def scrape_categories(catagory_products):\n",
        "    \"\"\"Scrape product data for each category.\"\"\"\n",
        "    for category_name, products in catagory_products.items():\n",
        "        print(f\"Scraping {category_name}...\")\n",
        "        for url in products[:30]:\n",
        "            full_url = 'https://www.netmeds.com/' + url\n",
        "            extract_data(full_url, category_name)\n",
        "        print(f\"Scraping {category_name} completed.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJRtidPnrkFh"
      },
      "outputs": [],
      "source": [
        "\n",
        "scrape_categories(catagory_products)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7qygCTGAL0M"
      },
      "outputs": [],
      "source": [
        "!du -sh product_images\n",
        "!du -sh catagory\n",
        "!du -sh product_data.csv\n",
        "!du -sh catagory_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ-lKqPoAXUI"
      },
      "outputs": [],
      "source": [
        "!du -sh product_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlmud7UMsRhf"
      },
      "outputs": [],
      "source": [
        "# !ls\n",
        "!rm -rf product_images\n",
        "!rm -rf product_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNAx9PAsF3zD"
      },
      "outputs": [],
      "source": [
        "!cp -r product_images drive/MyDrive/sc\n",
        "!cp -r catagory drive/MyDrive/sc\n",
        "!cp -r product_data.csv drive/MyDrive/sc\n",
        "!cp -r catagory_data.csv drive/MyDrive/sc"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A1xYmvNL4ffBvuSu6VqqCwWWUXmJd7x6",
      "authorship_tag": "ABX9TyPAm3vHVfDTX+gheahixOuX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}